{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9877061,"sourceType":"datasetVersion","datasetId":6063935},{"sourceId":9995250,"sourceType":"datasetVersion","datasetId":6151897}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:14.319513Z","iopub.execute_input":"2024-11-24T00:10:14.319786Z","iopub.status.idle":"2024-11-24T00:10:18.183921Z","shell.execute_reply.started":"2024-11-24T00:10:14.319760Z","shell.execute_reply":"2024-11-24T00:10:18.182993Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house-b-csv/house_b_combined_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:18.185295Z","iopub.execute_input":"2024-11-24T00:10:18.185718Z","iopub.status.idle":"2024-11-24T00:10:21.935935Z","shell.execute_reply.started":"2024-11-24T00:10:18.185691Z","shell.execute_reply":"2024-11-24T00:10:21.935223Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Assuming 'df' is the DataFrame that contains your data\n# Preprocessing: Drop the Hour, Resident columns for simplicity\ndata = df.drop(columns=['Hour', 'Resident1', 'Resident2']).values  # Remove non-sensor data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:21.936965Z","iopub.execute_input":"2024-11-24T00:10:21.937251Z","iopub.status.idle":"2024-11-24T00:10:22.081020Z","shell.execute_reply.started":"2024-11-24T00:10:21.937225Z","shell.execute_reply":"2024-11-24T00:10:22.079992Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Normalize or standardize data if needed\nscaler = StandardScaler()\ndata = scaler.fit_transform(data)  # Normalize sensor data (optional)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:22.083579Z","iopub.execute_input":"2024-11-24T00:10:22.083927Z","iopub.status.idle":"2024-11-24T00:10:22.903137Z","shell.execute_reply.started":"2024-11-24T00:10:22.083892Z","shell.execute_reply":"2024-11-24T00:10:22.902421Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Convert data to tensor\ndata_tensor = torch.tensor(data, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:22.904180Z","iopub.execute_input":"2024-11-24T00:10:22.904561Z","iopub.status.idle":"2024-11-24T00:10:23.049322Z","shell.execute_reply.started":"2024-11-24T00:10:22.904503Z","shell.execute_reply":"2024-11-24T00:10:23.048395Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Create Dataset and DataLoader for batch processing\nbatch_size = 2**17  # Adjust based on GPU memory availability\ndataset = TensorDataset(data_tensor)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.050435Z","iopub.execute_input":"2024-11-24T00:10:23.050967Z","iopub.status.idle":"2024-11-24T00:10:23.056419Z","shell.execute_reply.started":"2024-11-24T00:10:23.050938Z","shell.execute_reply":"2024-11-24T00:10:23.055579Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTM_Autoencoder(nn.Module):\n    def __init__(self, input_size, hidden_size, seq_len):\n        super(LSTM_Autoencoder, self).__init__()\n        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.output_layer = nn.Linear(hidden_size, input_size)\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n\n    def forward(self, x):\n        # Initialize hidden and cell states for encoder\n        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n        \n        # Pass through encoder\n        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n        \n        # Pass through decoder\n        decoded, _ = self.decoder(encoded, (hn, cn))\n        \n        # Map decoded output back to input size\n        decoded = self.output_layer(decoded)\n        \n        return decoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.057648Z","iopub.execute_input":"2024-11-24T00:10:23.057997Z","iopub.status.idle":"2024-11-24T00:10:23.069151Z","shell.execute_reply.started":"2024-11-24T00:10:23.057962Z","shell.execute_reply":"2024-11-24T00:10:23.068374Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Model Hyperparameters\ninput_size = data.shape[1]  # Number of sensor features\nhidden_size = 64  # You can adjust this based on your model's complexity\nseq_len = 1  # We are feeding one time step at a time (if you want multi-step sequence, change this)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.070179Z","iopub.execute_input":"2024-11-24T00:10:23.070437Z","iopub.status.idle":"2024-11-24T00:10:23.082074Z","shell.execute_reply.started":"2024-11-24T00:10:23.070409Z","shell.execute_reply":"2024-11-24T00:10:23.081211Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize the model and move it to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.083065Z","iopub.execute_input":"2024-11-24T00:10:23.083296Z","iopub.status.idle":"2024-11-24T00:10:23.392669Z","shell.execute_reply.started":"2024-11-24T00:10:23.083273Z","shell.execute_reply":"2024-11-24T00:10:23.391701Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = nn.DataParallel(model)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.395016Z","iopub.execute_input":"2024-11-24T00:10:23.395300Z","iopub.status.idle":"2024-11-24T00:10:23.401225Z","shell.execute_reply.started":"2024-11-24T00:10:23.395274Z","shell.execute_reply":"2024-11-24T00:10:23.400344Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Loss and Optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:23.402315Z","iopub.execute_input":"2024-11-24T00:10:23.402669Z","iopub.status.idle":"2024-11-24T00:10:24.178088Z","shell.execute_reply.started":"2024-11-24T00:10:23.402632Z","shell.execute_reply":"2024-11-24T00:10:24.177372Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize GradScaler for mixed precision training\nscaler = GradScaler()\n\nnum_epochs = 300  # Adjust as needed\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (batch_data,) in enumerate(data_loader):\n        batch_data = batch_data.to(device)\n        batch_data = batch_data.unsqueeze(1)  # Ensure correct shape\n\n        # Forward and backward pass under autocast for mixed precision\n        with autocast():\n            output = model(batch_data)\n            loss = criterion(output, batch_data)\n        \n        # Scale loss to prevent underflow and backpropagate\n        scaler.scale(loss).backward()\n        \n        # Optimizer step with scaled gradients\n        scaler.step(optimizer)\n        scaler.update()  # Update scaler for next iteration\n        optimizer.zero_grad()  # Reset gradients\n        \n        running_loss += loss.item()\n    \n    avg_loss = running_loss / len(data_loader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T00:10:24.179114Z","iopub.execute_input":"2024-11-24T00:10:24.179483Z","iopub.status.idle":"2024-11-24T02:02:29.740099Z","shell.execute_reply.started":"2024-11-24T00:10:24.179457Z","shell.execute_reply":"2024-11-24T02:02:29.738814Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3145811936.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_30/3145811936.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/300], Loss: 0.9816\nEpoch [2/300], Loss: 0.9058\nEpoch [3/300], Loss: 0.7740\nEpoch [4/300], Loss: 0.6337\nEpoch [5/300], Loss: 0.5102\nEpoch [6/300], Loss: 0.4235\nEpoch [7/300], Loss: 0.3637\nEpoch [8/300], Loss: 0.3151\nEpoch [9/300], Loss: 0.2763\nEpoch [10/300], Loss: 0.2456\nEpoch [11/300], Loss: 0.2215\nEpoch [12/300], Loss: 0.2001\nEpoch [13/300], Loss: 0.1812\nEpoch [14/300], Loss: 0.1642\nEpoch [15/300], Loss: 0.1491\nEpoch [16/300], Loss: 0.1354\nEpoch [17/300], Loss: 0.1236\nEpoch [18/300], Loss: 0.1142\nEpoch [19/300], Loss: 0.1056\nEpoch [20/300], Loss: 0.0982\nEpoch [21/300], Loss: 0.0916\nEpoch [22/300], Loss: 0.0854\nEpoch [23/300], Loss: 0.0799\nEpoch [24/300], Loss: 0.0749\nEpoch [25/300], Loss: 0.0700\nEpoch [26/300], Loss: 0.0657\nEpoch [27/300], Loss: 0.0618\nEpoch [28/300], Loss: 0.0581\nEpoch [29/300], Loss: 0.0546\nEpoch [30/300], Loss: 0.0513\nEpoch [31/300], Loss: 0.0484\nEpoch [32/300], Loss: 0.0457\nEpoch [33/300], Loss: 0.0432\nEpoch [34/300], Loss: 0.0408\nEpoch [35/300], Loss: 0.0387\nEpoch [36/300], Loss: 0.0365\nEpoch [37/300], Loss: 0.0347\nEpoch [38/300], Loss: 0.0329\nEpoch [39/300], Loss: 0.0312\nEpoch [40/300], Loss: 0.0296\nEpoch [41/300], Loss: 0.0281\nEpoch [42/300], Loss: 0.0267\nEpoch [43/300], Loss: 0.0254\nEpoch [44/300], Loss: 0.0241\nEpoch [45/300], Loss: 0.0229\nEpoch [46/300], Loss: 0.0218\nEpoch [47/300], Loss: 0.0208\nEpoch [48/300], Loss: 0.0198\nEpoch [49/300], Loss: 0.0189\nEpoch [50/300], Loss: 0.0181\nEpoch [51/300], Loss: 0.0173\nEpoch [52/300], Loss: 0.0165\nEpoch [53/300], Loss: 0.0158\nEpoch [54/300], Loss: 0.0151\nEpoch [55/300], Loss: 0.0145\nEpoch [56/300], Loss: 0.0139\nEpoch [57/300], Loss: 0.0134\nEpoch [58/300], Loss: 0.0129\nEpoch [59/300], Loss: 0.0124\nEpoch [60/300], Loss: 0.0119\nEpoch [61/300], Loss: 0.0115\nEpoch [62/300], Loss: 0.0112\nEpoch [63/300], Loss: 0.0107\nEpoch [64/300], Loss: 0.0104\nEpoch [65/300], Loss: 0.0100\nEpoch [66/300], Loss: 0.0097\nEpoch [67/300], Loss: 0.0094\nEpoch [68/300], Loss: 0.0091\nEpoch [69/300], Loss: 0.0088\nEpoch [70/300], Loss: 0.0086\nEpoch [71/300], Loss: 0.0083\nEpoch [72/300], Loss: 0.0081\nEpoch [73/300], Loss: 0.0078\nEpoch [74/300], Loss: 0.0077\nEpoch [75/300], Loss: 0.0075\nEpoch [76/300], Loss: 0.0073\nEpoch [77/300], Loss: 0.0071\nEpoch [78/300], Loss: 0.0069\nEpoch [79/300], Loss: 0.0068\nEpoch [80/300], Loss: 0.0066\nEpoch [81/300], Loss: 0.0065\nEpoch [82/300], Loss: 0.0062\nEpoch [83/300], Loss: 0.0061\nEpoch [84/300], Loss: 0.0060\nEpoch [85/300], Loss: 0.0060\nEpoch [86/300], Loss: 0.0058\nEpoch [87/300], Loss: 0.0056\nEpoch [88/300], Loss: 0.0055\nEpoch [89/300], Loss: 0.0054\nEpoch [90/300], Loss: 0.0053\nEpoch [91/300], Loss: 0.0052\nEpoch [92/300], Loss: 0.0051\nEpoch [93/300], Loss: 0.0050\nEpoch [94/300], Loss: 0.0049\nEpoch [95/300], Loss: 0.0048\nEpoch [96/300], Loss: 0.0048\nEpoch [97/300], Loss: 0.0047\nEpoch [98/300], Loss: 0.0046\nEpoch [99/300], Loss: 0.0045\nEpoch [100/300], Loss: 0.0045\nEpoch [101/300], Loss: 0.0044\nEpoch [102/300], Loss: 0.0043\nEpoch [103/300], Loss: 0.0042\nEpoch [104/300], Loss: 0.0042\nEpoch [105/300], Loss: 0.0041\nEpoch [106/300], Loss: 0.0041\nEpoch [107/300], Loss: 0.0040\nEpoch [108/300], Loss: 0.0039\nEpoch [109/300], Loss: 0.0039\nEpoch [110/300], Loss: 0.0039\nEpoch [111/300], Loss: 0.0038\nEpoch [112/300], Loss: 0.0037\nEpoch [113/300], Loss: 0.0037\nEpoch [114/300], Loss: 0.0036\nEpoch [115/300], Loss: 0.0036\nEpoch [116/300], Loss: 0.0036\nEpoch [117/300], Loss: 0.0035\nEpoch [118/300], Loss: 0.0035\nEpoch [119/300], Loss: 0.0034\nEpoch [120/300], Loss: 0.0033\nEpoch [121/300], Loss: 0.0033\nEpoch [122/300], Loss: 0.0033\nEpoch [123/300], Loss: 0.0032\nEpoch [124/300], Loss: 0.0032\nEpoch [125/300], Loss: 0.0031\nEpoch [126/300], Loss: 0.0031\nEpoch [127/300], Loss: 0.0031\nEpoch [128/300], Loss: 0.0031\nEpoch [129/300], Loss: 0.0030\nEpoch [130/300], Loss: 0.0030\nEpoch [131/300], Loss: 0.0029\nEpoch [132/300], Loss: 0.0029\nEpoch [133/300], Loss: 0.0028\nEpoch [134/300], Loss: 0.0028\nEpoch [135/300], Loss: 0.0028\nEpoch [136/300], Loss: 0.0028\nEpoch [137/300], Loss: 0.0027\nEpoch [138/300], Loss: 0.0027\nEpoch [139/300], Loss: 0.0026\nEpoch [140/300], Loss: 0.0026\nEpoch [141/300], Loss: 0.0026\nEpoch [142/300], Loss: 0.0026\nEpoch [143/300], Loss: 0.0025\nEpoch [144/300], Loss: 0.0025\nEpoch [145/300], Loss: 0.0025\nEpoch [146/300], Loss: 0.0024\nEpoch [147/300], Loss: 0.0024\nEpoch [148/300], Loss: 0.0024\nEpoch [149/300], Loss: 0.0024\nEpoch [150/300], Loss: 0.0023\nEpoch [151/300], Loss: 0.0023\nEpoch [152/300], Loss: 0.0023\nEpoch [153/300], Loss: 0.0023\nEpoch [154/300], Loss: 0.0023\nEpoch [155/300], Loss: 0.0022\nEpoch [156/300], Loss: 0.0022\nEpoch [157/300], Loss: 0.0023\nEpoch [158/300], Loss: 0.0022\nEpoch [159/300], Loss: 0.0021\nEpoch [160/300], Loss: 0.0021\nEpoch [161/300], Loss: 0.0021\nEpoch [162/300], Loss: 0.0021\nEpoch [163/300], Loss: 0.0021\nEpoch [164/300], Loss: 0.0021\nEpoch [165/300], Loss: 0.0020\nEpoch [166/300], Loss: 0.0021\nEpoch [167/300], Loss: 0.0020\nEpoch [168/300], Loss: 0.0020\nEpoch [169/300], Loss: 0.0020\nEpoch [170/300], Loss: 0.0020\nEpoch [171/300], Loss: 0.0019\nEpoch [172/300], Loss: 0.0019\nEpoch [173/300], Loss: 0.0019\nEpoch [174/300], Loss: 0.0019\nEpoch [175/300], Loss: 0.0019\nEpoch [176/300], Loss: 0.0019\nEpoch [177/300], Loss: 0.0018\nEpoch [178/300], Loss: 0.0018\nEpoch [179/300], Loss: 0.0018\nEpoch [180/300], Loss: 0.0018\nEpoch [181/300], Loss: 0.0018\nEpoch [182/300], Loss: 0.0018\nEpoch [183/300], Loss: 0.0017\nEpoch [184/300], Loss: 0.0017\nEpoch [185/300], Loss: 0.0017\nEpoch [186/300], Loss: 0.0017\nEpoch [187/300], Loss: 0.0017\nEpoch [188/300], Loss: 0.0017\nEpoch [189/300], Loss: 0.0017\nEpoch [190/300], Loss: 0.0016\nEpoch [191/300], Loss: 0.0017\nEpoch [192/300], Loss: 0.0016\nEpoch [193/300], Loss: 0.0016\nEpoch [194/300], Loss: 0.0016\nEpoch [195/300], Loss: 0.0016\nEpoch [196/300], Loss: 0.0016\nEpoch [197/300], Loss: 0.0016\nEpoch [198/300], Loss: 0.0016\nEpoch [199/300], Loss: 0.0015\nEpoch [200/300], Loss: 0.0015\nEpoch [201/300], Loss: 0.0015\nEpoch [202/300], Loss: 0.0016\nEpoch [203/300], Loss: 0.0015\nEpoch [204/300], Loss: 0.0015\nEpoch [205/300], Loss: 0.0015\nEpoch [206/300], Loss: 0.0015\nEpoch [207/300], Loss: 0.0015\nEpoch [208/300], Loss: 0.0015\nEpoch [209/300], Loss: 0.0015\nEpoch [210/300], Loss: 0.0014\nEpoch [211/300], Loss: 0.0015\nEpoch [212/300], Loss: 0.0014\nEpoch [213/300], Loss: 0.0014\nEpoch [214/300], Loss: 0.0014\nEpoch [215/300], Loss: 0.0014\nEpoch [216/300], Loss: 0.0014\nEpoch [217/300], Loss: 0.0014\nEpoch [218/300], Loss: 0.0014\nEpoch [219/300], Loss: 0.0013\nEpoch [220/300], Loss: 0.0014\nEpoch [221/300], Loss: 0.0014\nEpoch [222/300], Loss: 0.0013\nEpoch [223/300], Loss: 0.0013\nEpoch [224/300], Loss: 0.0013\nEpoch [225/300], Loss: 0.0013\nEpoch [226/300], Loss: 0.0013\nEpoch [227/300], Loss: 0.0013\nEpoch [228/300], Loss: 0.0013\nEpoch [229/300], Loss: 0.0013\nEpoch [230/300], Loss: 0.0013\nEpoch [231/300], Loss: 0.0012\nEpoch [232/300], Loss: 0.0013\nEpoch [233/300], Loss: 0.0013\nEpoch [234/300], Loss: 0.0012\nEpoch [235/300], Loss: 0.0012\nEpoch [236/300], Loss: 0.0012\nEpoch [237/300], Loss: 0.0012\nEpoch [238/300], Loss: 0.0012\nEpoch [239/300], Loss: 0.0012\nEpoch [240/300], Loss: 0.0012\nEpoch [241/300], Loss: 0.0012\nEpoch [242/300], Loss: 0.0012\nEpoch [243/300], Loss: 0.0011\nEpoch [244/300], Loss: 0.0012\nEpoch [245/300], Loss: 0.0011\nEpoch [246/300], Loss: 0.0011\nEpoch [247/300], Loss: 0.0012\nEpoch [248/300], Loss: 0.0011\nEpoch [249/300], Loss: 0.0011\nEpoch [250/300], Loss: 0.0011\nEpoch [251/300], Loss: 0.0011\nEpoch [252/300], Loss: 0.0011\nEpoch [253/300], Loss: 0.0011\nEpoch [254/300], Loss: 0.0011\nEpoch [255/300], Loss: 0.0011\nEpoch [256/300], Loss: 0.0011\nEpoch [257/300], Loss: 0.0011\nEpoch [258/300], Loss: 0.0010\nEpoch [259/300], Loss: 0.0010\nEpoch [260/300], Loss: 0.0010\nEpoch [261/300], Loss: 0.0011\nEpoch [262/300], Loss: 0.0010\nEpoch [263/300], Loss: 0.0010\nEpoch [264/300], Loss: 0.0010\nEpoch [265/300], Loss: 0.0010\nEpoch [266/300], Loss: 0.0010\nEpoch [267/300], Loss: 0.0010\nEpoch [268/300], Loss: 0.0010\nEpoch [269/300], Loss: 0.0010\nEpoch [270/300], Loss: 0.0010\nEpoch [271/300], Loss: 0.0010\nEpoch [272/300], Loss: 0.0009\nEpoch [273/300], Loss: 0.0009\nEpoch [274/300], Loss: 0.0010\nEpoch [275/300], Loss: 0.0010\nEpoch [276/300], Loss: 0.0009\nEpoch [277/300], Loss: 0.0009\nEpoch [278/300], Loss: 0.0009\nEpoch [279/300], Loss: 0.0009\nEpoch [280/300], Loss: 0.0009\nEpoch [281/300], Loss: 0.0009\nEpoch [282/300], Loss: 0.0009\nEpoch [283/300], Loss: 0.0009\nEpoch [284/300], Loss: 0.0009\nEpoch [285/300], Loss: 0.0009\nEpoch [286/300], Loss: 0.0009\nEpoch [287/300], Loss: 0.0009\nEpoch [288/300], Loss: 0.0009\nEpoch [289/300], Loss: 0.0009\nEpoch [290/300], Loss: 0.0009\nEpoch [291/300], Loss: 0.0009\nEpoch [292/300], Loss: 0.0009\nEpoch [293/300], Loss: 0.0008\nEpoch [294/300], Loss: 0.0008\nEpoch [295/300], Loss: 0.0008\nEpoch [296/300], Loss: 0.0008\nEpoch [297/300], Loss: 0.0008\nEpoch [298/300], Loss: 0.0008\nEpoch [299/300], Loss: 0.0008\nEpoch [300/300], Loss: 0.0008\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nfrom torch.cuda.amp import autocast\n\nmodel.eval()\nreconstruction_errors = []\n\nwith torch.no_grad():\n    for batch_idx, (batch_data,) in enumerate(data_loader):\n        batch_data = batch_data.to(device)\n        batch_data = batch_data.unsqueeze(1)  \n\n        with autocast():\n            output = model(batch_data)\n\n        error = torch.mean((batch_data - output) ** 2, dim=[1, 2]).cpu().numpy()\n        reconstruction_errors.extend(error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:29.741146Z","iopub.execute_input":"2024-11-24T02:02:29.741433Z","iopub.status.idle":"2024-11-24T02:02:52.314168Z","shell.execute_reply.started":"2024-11-24T02:02:29.741403Z","shell.execute_reply":"2024-11-24T02:02:52.313117Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2326915126.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"threshold = np.percentile(reconstruction_errors, 95)\n\nanomalies = np.array(reconstruction_errors) > threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:52.315361Z","iopub.execute_input":"2024-11-24T02:02:52.316223Z","iopub.status.idle":"2024-11-24T02:02:52.573848Z","shell.execute_reply.started":"2024-11-24T02:02:52.316180Z","shell.execute_reply":"2024-11-24T02:02:52.572801Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"total_anomalies = (anomalies==True).sum()\ntotal_anomalies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:52.575027Z","iopub.execute_input":"2024-11-24T02:02:52.575302Z","iopub.status.idle":"2024-11-24T02:02:52.583902Z","shell.execute_reply.started":"2024-11-24T02:02:52.575277Z","shell.execute_reply":"2024-11-24T02:02:52.582991Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"129597"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"(anomalies==False).sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:52.584939Z","iopub.execute_input":"2024-11-24T02:02:52.585285Z","iopub.status.idle":"2024-11-24T02:02:52.592038Z","shell.execute_reply.started":"2024-11-24T02:02:52.585258Z","shell.execute_reply":"2024-11-24T02:02:52.591315Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"2462403"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df[\"Anomaly\"] = anomalies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:52.592974Z","iopub.execute_input":"2024-11-24T02:02:52.593211Z","iopub.status.idle":"2024-11-24T02:02:52.598327Z","shell.execute_reply.started":"2024-11-24T02:02:52.593188Z","shell.execute_reply":"2024-11-24T02:02:52.597588Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/house_b_detected_anomalies.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:02:52.599314Z","iopub.execute_input":"2024-11-24T02:02:52.599606Z","iopub.status.idle":"2024-11-24T02:03:04.002244Z","shell.execute_reply.started":"2024-11-24T02:02:52.599577Z","shell.execute_reply":"2024-11-24T02:03:04.001577Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:03:04.003096Z","iopub.execute_input":"2024-11-24T02:03:04.003336Z","iopub.status.idle":"2024-11-24T02:03:04.008599Z","shell.execute_reply.started":"2024-11-24T02:03:04.003313Z","shell.execute_reply":"2024-11-24T02:03:04.007796Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"2592000"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"print(f\"Percentage Anomalies = {(total_anomalies/len(df))*100}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:03:04.009629Z","iopub.execute_input":"2024-11-24T02:03:04.009930Z","iopub.status.idle":"2024-11-24T02:03:04.015752Z","shell.execute_reply.started":"2024-11-24T02:03:04.009888Z","shell.execute_reply":"2024-11-24T02:03:04.014938Z"}},"outputs":[{"name":"stdout","text":"Percentage Anomalies = 4.999884259259259%\n","output_type":"stream"}],"execution_count":20}]}