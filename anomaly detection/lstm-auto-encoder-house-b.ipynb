{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9877061,"sourceType":"datasetVersion","datasetId":6063935},{"sourceId":9995250,"sourceType":"datasetVersion","datasetId":6151897}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:20.157212Z","iopub.execute_input":"2024-11-25T23:06:20.157622Z","iopub.status.idle":"2024-11-25T23:06:20.162543Z","shell.execute_reply.started":"2024-11-25T23:06:20.157590Z","shell.execute_reply":"2024-11-25T23:06:20.161547Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/d/jayjajoo01/house-b-csv/house_b_combined_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:20.166492Z","iopub.execute_input":"2024-11-25T23:06:20.166750Z","iopub.status.idle":"2024-11-25T23:06:22.850290Z","shell.execute_reply.started":"2024-11-25T23:06:20.166726Z","shell.execute_reply":"2024-11-25T23:06:22.849284Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Assuming 'df' is the DataFrame that contains your data\n# Preprocessing: Drop the Hour, Resident columns for simplicity\ndata = df.drop(columns=['Unnamed: 0','Hour', 'Resident1', 'Resident2']) # Remove non-sensor data\nprint(data.columns)\ndata = data.values ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:22.851959Z","iopub.execute_input":"2024-11-25T23:06:22.852239Z","iopub.status.idle":"2024-11-25T23:06:22.985300Z","shell.execute_reply.started":"2024-11-25T23:06:22.852212Z","shell.execute_reply":"2024-11-25T23:06:22.984324Z"}},"outputs":[{"name":"stdout","text":"Index(['contact_kitchen_cupboard_1', 'contact_kitchen_cupboard_2',\n       'contact_house_door', 'contact_wardrobe_door_1',\n       'contact_wardrobe_door_2', 'contact_shower_door', 'distance_tap',\n       'force_chair_1', 'force_chair_2', 'force_chair_3', 'photocell_fridge',\n       'photocell_kitchen_drawer', 'pressure_mat_couch_1',\n       'pressure_mat_couch_2', 'pressure_mat_bed_1', 'pressure_mat_bed_2',\n       'pressure_mat_armchair', 'sonar_bathroom_door', 'sonar_kitchen',\n       'sonar_closet'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Normalize or standardize data if needed\nsc = StandardScaler()\ndata = sc.fit_transform(data)  # Normalize sensor data (optional)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:22.986518Z","iopub.execute_input":"2024-11-25T23:06:22.987209Z","iopub.status.idle":"2024-11-25T23:06:23.724371Z","shell.execute_reply.started":"2024-11-25T23:06:22.987169Z","shell.execute_reply":"2024-11-25T23:06:23.723468Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from joblib import dump\n\ndump(sc, \"/kaggle/working/standardscaler_hb.joblib\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.725601Z","iopub.execute_input":"2024-11-25T23:06:23.726067Z","iopub.status.idle":"2024-11-25T23:06:23.733164Z","shell.execute_reply.started":"2024-11-25T23:06:23.726028Z","shell.execute_reply":"2024-11-25T23:06:23.732023Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/standardscaler_hb.joblib']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Convert data to tensor\ndata_tensor = torch.tensor(data, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.735858Z","iopub.execute_input":"2024-11-25T23:06:23.736134Z","iopub.status.idle":"2024-11-25T23:06:23.775511Z","shell.execute_reply.started":"2024-11-25T23:06:23.736091Z","shell.execute_reply":"2024-11-25T23:06:23.774834Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Create Dataset and DataLoader for batch processing\nbatch_size = 2**17  # Adjust based on GPU memory availability\ndataset = TensorDataset(data_tensor)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.776503Z","iopub.execute_input":"2024-11-25T23:06:23.776823Z","iopub.status.idle":"2024-11-25T23:06:23.781117Z","shell.execute_reply.started":"2024-11-25T23:06:23.776786Z","shell.execute_reply":"2024-11-25T23:06:23.780311Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTM_Autoencoder(nn.Module):\n    def __init__(self, input_size, hidden_size, seq_len):\n        super(LSTM_Autoencoder, self).__init__()\n        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.output_layer = nn.Linear(hidden_size, input_size)\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n\n    def forward(self, x):\n        # Initialize hidden and cell states for encoder\n        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n        \n        # Pass through encoder\n        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n        \n        # Pass through decoder\n        decoded, _ = self.decoder(encoded, (hn, cn))\n        \n        # Map decoded output back to input size\n        decoded = self.output_layer(decoded)\n        \n        return decoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.782299Z","iopub.execute_input":"2024-11-25T23:06:23.782837Z","iopub.status.idle":"2024-11-25T23:06:23.792844Z","shell.execute_reply.started":"2024-11-25T23:06:23.782800Z","shell.execute_reply":"2024-11-25T23:06:23.792046Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Model Hyperparameters\ninput_size = data.shape[1]  # Number of sensor features\nhidden_size = 256  # You can adjust this based on your model's complexity\nseq_len = 1  # We are feeding one time step at a time (if you want multi-step sequence, change this)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.793890Z","iopub.execute_input":"2024-11-25T23:06:23.794214Z","iopub.status.idle":"2024-11-25T23:06:23.802015Z","shell.execute_reply.started":"2024-11-25T23:06:23.794174Z","shell.execute_reply":"2024-11-25T23:06:23.801254Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Initialize the model and move it to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.803003Z","iopub.execute_input":"2024-11-25T23:06:23.803280Z","iopub.status.idle":"2024-11-25T23:06:23.819965Z","shell.execute_reply.started":"2024-11-25T23:06:23.803237Z","shell.execute_reply":"2024-11-25T23:06:23.819328Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model = nn.DataParallel(model)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.820943Z","iopub.execute_input":"2024-11-25T23:06:23.821257Z","iopub.status.idle":"2024-11-25T23:06:23.827082Z","shell.execute_reply.started":"2024-11-25T23:06:23.821224Z","shell.execute_reply":"2024-11-25T23:06:23.826327Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Loss and Optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.828144Z","iopub.execute_input":"2024-11-25T23:06:23.828467Z","iopub.status.idle":"2024-11-25T23:06:23.833750Z","shell.execute_reply.started":"2024-11-25T23:06:23.828431Z","shell.execute_reply":"2024-11-25T23:06:23.832783Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Initialize GradScaler for mixed precision training\nscaler = GradScaler()\n\nnum_epochs = 100  # Adjust as needed\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (batch_data,) in enumerate(data_loader):\n        batch_data = batch_data.to(device)\n        batch_data = batch_data.unsqueeze(1)  # Ensure correct shape\n\n        # Forward and backward pass under autocast for mixed precision\n        with autocast():\n            output = model(batch_data)\n            loss = criterion(output, batch_data)\n        \n        # Scale loss to prevent underflow and backpropagate\n        scaler.scale(loss).backward()\n        \n        # Optimizer step with scaled gradients\n        scaler.step(optimizer)\n        scaler.update()  # Update scaler for next iteration\n        optimizer.zero_grad()  # Reset gradients\n        \n        running_loss += loss.item()\n    \n    avg_loss = running_loss / len(data_loader)\n    if((epoch+1)%5==0):\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:06:23.835707Z","iopub.execute_input":"2024-11-25T23:06:23.835976Z","iopub.status.idle":"2024-11-25T23:50:22.370460Z","shell.execute_reply.started":"2024-11-25T23:06:23.835953Z","shell.execute_reply":"2024-11-25T23:50:22.369670Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1740040032.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_30/1740040032.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/100], Loss: 0.0963\nEpoch [10/100], Loss: 0.0209\nEpoch [15/100], Loss: 0.0085\nEpoch [20/100], Loss: 0.0045\nEpoch [25/100], Loss: 0.0029\nEpoch [30/100], Loss: 0.0021\nEpoch [35/100], Loss: 0.0017\nEpoch [40/100], Loss: 0.0013\nEpoch [45/100], Loss: 0.0011\nEpoch [50/100], Loss: 0.0010\nEpoch [55/100], Loss: 0.0008\nEpoch [60/100], Loss: 0.0007\nEpoch [65/100], Loss: 0.0006\nEpoch [70/100], Loss: 0.0005\nEpoch [75/100], Loss: 0.0005\nEpoch [80/100], Loss: 0.0004\nEpoch [85/100], Loss: 0.0004\nEpoch [90/100], Loss: 0.0004\nEpoch [95/100], Loss: 0.0003\nEpoch [100/100], Loss: 0.0003\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import numpy as np\nfrom torch.cuda.amp import autocast\n\nmodel.eval()\nreconstruction_errors = []\n\nwith torch.no_grad():\n    for batch_idx, (batch_data,) in enumerate(data_loader):\n        batch_data = batch_data.to(device)\n        batch_data = batch_data.unsqueeze(1)  \n\n        with autocast():\n            output = model(batch_data)\n\n        error = torch.mean((batch_data - output) ** 2, dim=[1, 2]).cpu().numpy()\n        reconstruction_errors.extend(error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:22.371334Z","iopub.execute_input":"2024-11-25T23:50:22.371599Z","iopub.status.idle":"2024-11-25T23:50:48.883504Z","shell.execute_reply.started":"2024-11-25T23:50:22.371569Z","shell.execute_reply":"2024-11-25T23:50:48.882593Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2326915126.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"threshold = np.percentile(reconstruction_errors, 95)\n\nanomalies = np.array(reconstruction_errors) > threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:48.886977Z","iopub.execute_input":"2024-11-25T23:50:48.887286Z","iopub.status.idle":"2024-11-25T23:50:49.173786Z","shell.execute_reply.started":"2024-11-25T23:50:48.887256Z","shell.execute_reply":"2024-11-25T23:50:49.172834Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"total_anomalies = (anomalies==True).sum()\ntotal_anomalies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:49.174786Z","iopub.execute_input":"2024-11-25T23:50:49.175049Z","iopub.status.idle":"2024-11-25T23:50:49.182114Z","shell.execute_reply.started":"2024-11-25T23:50:49.175025Z","shell.execute_reply":"2024-11-25T23:50:49.181332Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"69245"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"(anomalies==False).sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:49.183345Z","iopub.execute_input":"2024-11-25T23:50:49.183876Z","iopub.status.idle":"2024-11-25T23:50:49.191952Z","shell.execute_reply.started":"2024-11-25T23:50:49.183838Z","shell.execute_reply":"2024-11-25T23:50:49.191184Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"2522755"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"df[\"Anomaly\"] = anomalies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:49.192994Z","iopub.execute_input":"2024-11-25T23:50:49.193349Z","iopub.status.idle":"2024-11-25T23:50:49.198846Z","shell.execute_reply.started":"2024-11-25T23:50:49.193308Z","shell.execute_reply":"2024-11-25T23:50:49.198016Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/house_b_detected_anomalies.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:50:49.199736Z","iopub.execute_input":"2024-11-25T23:50:49.199950Z","iopub.status.idle":"2024-11-25T23:51:00.632699Z","shell.execute_reply.started":"2024-11-25T23:50:49.199927Z","shell.execute_reply":"2024-11-25T23:51:00.631963Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:51:00.633870Z","iopub.execute_input":"2024-11-25T23:51:00.634622Z","iopub.status.idle":"2024-11-25T23:51:00.639922Z","shell.execute_reply.started":"2024-11-25T23:51:00.634581Z","shell.execute_reply":"2024-11-25T23:51:00.639020Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"2592000"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"print(f\"Percentage Anomalies = {(total_anomalies/len(df))*100}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:51:00.641001Z","iopub.execute_input":"2024-11-25T23:51:00.641422Z","iopub.status.idle":"2024-11-25T23:51:00.650779Z","shell.execute_reply.started":"2024-11-25T23:51:00.641374Z","shell.execute_reply":"2024-11-25T23:51:00.649883Z"}},"outputs":[{"name":"stdout","text":"Percentage Anomalies = 2.671489197530864%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/lstm_autoencoder_hb.pth\")\nnp.save(\"/kaggle/working/reconstruction_errors_hb.npy\", reconstruction_errors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T23:51:00.651637Z","iopub.execute_input":"2024-11-25T23:51:00.651954Z","iopub.status.idle":"2024-11-25T23:51:00.779352Z","shell.execute_reply.started":"2024-11-25T23:51:00.651929Z","shell.execute_reply":"2024-11-25T23:51:00.778716Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"<h1>Testing it on custom Data</h1>","metadata":{}},{"cell_type":"code","source":"import torch\n# Remove \"module.\" prefix if present\nfrom collections import OrderedDict\n\n\nclass LSTM_Autoencoder(nn.Module):\n    def __init__(self, input_size, hidden_size, seq_len):\n        super(LSTM_Autoencoder, self).__init__()\n        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.output_layer = nn.Linear(hidden_size, input_size)\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n\n    def forward(self, x):\n        # Initialize hidden and cell states for encoder\n        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n        \n        # Pass through encoder\n        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n        \n        # Pass through decoder\n        decoded, _ = self.decoder(encoded, (hn, cn))\n        \n        # Map decoded output back to input size\n        decoded = self.output_layer(decoded)\n        \n        return decoded\n\n# Initialize the model (ensure the parameters match the saved model)\ninput_size = 20  # Adjust based on your data\nhidden_size = 256  # Adjust based on your saved model\nseq_len = 1      # Adjust based on your sequence length\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:18:29.689093Z","iopub.execute_input":"2024-11-26T00:18:29.689808Z","iopub.status.idle":"2024-11-26T00:18:29.707757Z","shell.execute_reply.started":"2024-11-26T00:18:29.689771Z","shell.execute_reply":"2024-11-26T00:18:29.706821Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Load the saved state_dict\nstate_dict = torch.load(\"/kaggle/working/lstm_autoencoder_hb.pth\", map_location=device)\n\n\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    new_key = k.replace(\"module.\", \"\")  # Remove 'module.' prefix\n    new_state_dict[new_key] = v\n\n# Load the updated state_dict into the model\nmodel.load_state_dict(new_state_dict)\n\n# Set the model to evaluation mode\nmodel.eval()\n\nprint(\"Model loaded successfully with adjusted keys!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:18:32.528087Z","iopub.execute_input":"2024-11-26T00:18:32.529073Z","iopub.status.idle":"2024-11-26T00:18:32.539553Z","shell.execute_reply.started":"2024-11-26T00:18:32.529033Z","shell.execute_reply":"2024-11-26T00:18:32.538543Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully with adjusted keys!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/381041762.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(\"/kaggle/working/lstm_autoencoder_hb.pth\", map_location=device)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"reconstruction_errors = np.load(\"/kaggle/working/reconstruction_errors_hb.npy\")\n\nthreshold = np.percentile(reconstruction_errors, 95)\nanomalies = reconstruction_errors > threshold\n\ntotal_anomalies = anomalies.sum()\nprint(f\"Total anomalies: {total_anomalies}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:18:36.639133Z","iopub.execute_input":"2024-11-26T00:18:36.639495Z","iopub.status.idle":"2024-11-26T00:18:36.706224Z","shell.execute_reply.started":"2024-11-26T00:18:36.639464Z","shell.execute_reply":"2024-11-26T00:18:36.705556Z"}},"outputs":[{"name":"stdout","text":"Total anomalies: 69245\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:18:38.873788Z","iopub.execute_input":"2024-11-26T00:18:38.874144Z","iopub.status.idle":"2024-11-26T00:18:38.879612Z","shell.execute_reply.started":"2024-11-26T00:18:38.874115Z","shell.execute_reply":"2024-11-26T00:18:38.878767Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.00016648862219881266"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"from joblib import load\n\n# Load the scaler from the file\nsc = load('/kaggle/working/standardscaler_hb.joblib')\nsc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:18:39.643766Z","iopub.execute_input":"2024-11-26T00:18:39.644096Z","iopub.status.idle":"2024-11-26T00:18:39.653048Z","shell.execute_reply.started":"2024-11-26T00:18:39.644066Z","shell.execute_reply":"2024-11-26T00:18:39.652222Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"StandardScaler()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# Replace with actual new data\nnew_data = np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n\n# Transform new data using the StandardScaler\nnew_data = sc.transform(new_data)\n\n# Convert to tensor and reshape for LSTM\nnew_data_tensor = torch.tensor(new_data, dtype=torch.float32).unsqueeze(0).to(device)  \n# Shape: (batch_size=1, sequence_length=1, input_size=22)\n\n# Forward pass through the model\nmodel.eval()\nwith torch.no_grad():\n    new_data_reconstructed = model(new_data_tensor)\n    reconstruction_error = torch.mean((new_data_tensor - new_data_reconstructed) ** 2).item()\n\n# Compare the error with the threshold\nis_anomaly = reconstruction_error > threshold\nprint(f\"Reconstruction Error: {reconstruction_error}, Anomaly: {is_anomaly}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:32:00.664892Z","iopub.execute_input":"2024-11-26T00:32:00.665729Z","iopub.status.idle":"2024-11-26T00:32:00.673888Z","shell.execute_reply.started":"2024-11-26T00:32:00.665694Z","shell.execute_reply":"2024-11-26T00:32:00.673129Z"}},"outputs":[{"name":"stdout","text":"Reconstruction Error: 4.435534719959833e-06, Anomaly: False\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}