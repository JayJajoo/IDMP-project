{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:17.544152Z",
     "iopub.status.busy": "2024-11-25T23:04:17.543877Z",
     "iopub.status.idle": "2024-11-25T23:04:21.731492Z",
     "shell.execute_reply": "2024-11-25T23:04:21.730798Z",
     "shell.execute_reply.started": "2024-11-25T23:04:17.544125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:21.733883Z",
     "iopub.status.busy": "2024-11-25T23:04:21.733087Z",
     "iopub.status.idle": "2024-11-25T23:04:25.403646Z",
     "shell.execute_reply": "2024-11-25T23:04:25.402915Z",
     "shell.execute_reply.started": "2024-11-25T23:04:21.733838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/house-a/house_a_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:25.404967Z",
     "iopub.status.busy": "2024-11-25T23:04:25.404676Z",
     "iopub.status.idle": "2024-11-25T23:04:25.543593Z",
     "shell.execute_reply": "2024-11-25T23:04:25.542684Z",
     "shell.execute_reply.started": "2024-11-25T23:04:25.404940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['photocell_wardrobe', 'photocell_couch', 'ir_tv_receiver',\n",
      "       'force_couch_1', 'force_couch_2', 'distance_chair_1',\n",
      "       'distance_chair_2', 'photocell_fridge', 'photocell_kitchen_drawer',\n",
      "       'photocell_wardrobe_2', 'photocell_bathroom_cabinet',\n",
      "       'contact_house_door', 'contact_bathroom_door', 'contact_shower_door',\n",
      "       'sonar_hall', 'sonar_kitchen', 'distance_tap', 'distance_water_closet',\n",
      "       'temperature_kitchen', 'force_bed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is the DataFrame that contains your data\n",
    "# Preprocessing: Drop the Hour, Resident columns for simplicity\n",
    "data = df.drop(columns=['Unnamed: 0','Hour', 'Resident1', 'Resident2']) # Remove non-sensor data\n",
    "print(data.columns)\n",
    "data = data.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:25.545430Z",
     "iopub.status.busy": "2024-11-25T23:04:25.545154Z",
     "iopub.status.idle": "2024-11-25T23:04:26.301392Z",
     "shell.execute_reply": "2024-11-25T23:04:26.300442Z",
     "shell.execute_reply.started": "2024-11-25T23:04:25.545403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize or standardize data if needed\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)  # Normalize sensor data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.302823Z",
     "iopub.status.busy": "2024-11-25T23:04:26.302513Z",
     "iopub.status.idle": "2024-11-25T23:04:26.311527Z",
     "shell.execute_reply": "2024-11-25T23:04:26.310697Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.302784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/standardscaler_ha.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(sc, \"/kaggle/working/standardscaler_ha.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.312702Z",
     "iopub.status.busy": "2024-11-25T23:04:26.312464Z",
     "iopub.status.idle": "2024-11-25T23:04:26.399246Z",
     "shell.execute_reply": "2024-11-25T23:04:26.398162Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.312678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.400440Z",
     "iopub.status.busy": "2024-11-25T23:04:26.400142Z",
     "iopub.status.idle": "2024-11-25T23:04:26.406390Z",
     "shell.execute_reply": "2024-11-25T23:04:26.405490Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.400409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader for batch processing\n",
    "batch_size = 2**17  # Adjust based on GPU memory availability\n",
    "dataset = TensorDataset(data_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.407901Z",
     "iopub.status.busy": "2024-11-25T23:04:26.407163Z",
     "iopub.status.idle": "2024-11-25T23:04:26.427599Z",
     "shell.execute_reply": "2024-11-25T23:04:26.426750Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.407869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seq_len):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for encoder\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # batch_size, hidden_size\n",
    "        \n",
    "        # Pass through encoder\n",
    "        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        \n",
    "        # Pass through decoder\n",
    "        decoded, _ = self.decoder(encoded, (hn, cn))\n",
    "        \n",
    "        # Map decoded output back to input size\n",
    "        decoded = self.output_layer(decoded)\n",
    "        \n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.428731Z",
     "iopub.status.busy": "2024-11-25T23:04:26.428452Z",
     "iopub.status.idle": "2024-11-25T23:04:26.442866Z",
     "shell.execute_reply": "2024-11-25T23:04:26.442100Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.428684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "input_size = data.shape[1]  # Number of sensor features\n",
    "hidden_size = 256  # You can adjust this based on your model's complexity\n",
    "seq_len = 1  # We are feeding one time step at a time (if you want multi-step sequence, change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.445771Z",
     "iopub.status.busy": "2024-11-25T23:04:26.445442Z",
     "iopub.status.idle": "2024-11-25T23:04:26.795344Z",
     "shell.execute_reply": "2024-11-25T23:04:26.794386Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.445712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model and move it to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.796736Z",
     "iopub.status.busy": "2024-11-25T23:04:26.796452Z",
     "iopub.status.idle": "2024-11-25T23:04:26.803638Z",
     "shell.execute_reply": "2024-11-25T23:04:26.802816Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.796708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:26.805231Z",
     "iopub.status.busy": "2024-11-25T23:04:26.804883Z",
     "iopub.status.idle": "2024-11-25T23:04:27.654595Z",
     "shell.execute_reply": "2024-11-25T23:04:27.653692Z",
     "shell.execute_reply.started": "2024-11-25T23:04:26.805182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:04:27.656120Z",
     "iopub.status.busy": "2024-11-25T23:04:27.655644Z",
     "iopub.status.idle": "2024-11-25T23:46:59.441171Z",
     "shell.execute_reply": "2024-11-25T23:46:59.439087Z",
     "shell.execute_reply.started": "2024-11-25T23:04:27.656078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1740040032.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_30/1740040032.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.0271\n",
      "Epoch [10/100], Loss: 0.0037\n",
      "Epoch [15/100], Loss: 0.0024\n",
      "Epoch [20/100], Loss: 0.0018\n",
      "Epoch [25/100], Loss: 0.0014\n",
      "Epoch [30/100], Loss: 0.0011\n",
      "Epoch [35/100], Loss: 0.0008\n",
      "Epoch [40/100], Loss: 0.0006\n",
      "Epoch [45/100], Loss: 0.0004\n",
      "Epoch [50/100], Loss: 0.0003\n",
      "Epoch [55/100], Loss: 0.0003\n",
      "Epoch [60/100], Loss: 0.0002\n",
      "Epoch [65/100], Loss: 0.0002\n",
      "Epoch [70/100], Loss: 0.0001\n",
      "Epoch [75/100], Loss: 0.0001\n",
      "Epoch [80/100], Loss: 0.0001\n",
      "Epoch [85/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_epochs = 100  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (batch_data,) in enumerate(data_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_data = batch_data.unsqueeze(1)  # Ensure correct shape\n",
    "\n",
    "        # Forward and backward pass under autocast for mixed precision\n",
    "        with autocast():\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_data)\n",
    "        \n",
    "        # Scale loss to prevent underflow and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Optimizer step with scaled gradients\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Update scaler for next iteration\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    if((epoch+1)%5==0):\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:46:59.442677Z",
     "iopub.status.busy": "2024-11-25T23:46:59.442380Z",
     "iopub.status.idle": "2024-11-25T23:47:24.900059Z",
     "shell.execute_reply": "2024-11-25T23:47:24.899262Z",
     "shell.execute_reply.started": "2024-11-25T23:46:59.442646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/2326915126.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "model.eval()\n",
    "reconstruction_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (batch_data,) in enumerate(data_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_data = batch_data.unsqueeze(1)  \n",
    "\n",
    "        with autocast():\n",
    "            output = model(batch_data)\n",
    "\n",
    "        error = torch.mean((batch_data - output) ** 2, dim=[1, 2]).cpu().numpy()\n",
    "        reconstruction_errors.extend(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:24.901879Z",
     "iopub.status.busy": "2024-11-25T23:47:24.901566Z",
     "iopub.status.idle": "2024-11-25T23:47:25.197292Z",
     "shell.execute_reply": "2024-11-25T23:47:25.196585Z",
     "shell.execute_reply.started": "2024-11-25T23:47:24.901847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "threshold = np.percentile(reconstruction_errors, 95)\n",
    "\n",
    "anomalies = np.array(reconstruction_errors) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:25.198597Z",
     "iopub.status.busy": "2024-11-25T23:47:25.198315Z",
     "iopub.status.idle": "2024-11-25T23:47:25.205406Z",
     "shell.execute_reply": "2024-11-25T23:47:25.204698Z",
     "shell.execute_reply.started": "2024-11-25T23:47:25.198570Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126928"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_anomalies = (anomalies==True).sum()\n",
    "total_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:25.206780Z",
     "iopub.status.busy": "2024-11-25T23:47:25.206473Z",
     "iopub.status.idle": "2024-11-25T23:47:25.216601Z",
     "shell.execute_reply": "2024-11-25T23:47:25.215831Z",
     "shell.execute_reply.started": "2024-11-25T23:47:25.206729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465072"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(anomalies==False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:25.217805Z",
     "iopub.status.busy": "2024-11-25T23:47:25.217474Z",
     "iopub.status.idle": "2024-11-25T23:47:25.222809Z",
     "shell.execute_reply": "2024-11-25T23:47:25.222138Z",
     "shell.execute_reply.started": "2024-11-25T23:47:25.217748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"Anomaly\"] = anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:25.223943Z",
     "iopub.status.busy": "2024-11-25T23:47:25.223679Z",
     "iopub.status.idle": "2024-11-25T23:47:35.939782Z",
     "shell.execute_reply": "2024-11-25T23:47:35.938986Z",
     "shell.execute_reply.started": "2024-11-25T23:47:25.223918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/kaggle/working/house_a_detected_anomalies.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:35.941261Z",
     "iopub.status.busy": "2024-11-25T23:47:35.940894Z",
     "iopub.status.idle": "2024-11-25T23:47:35.947025Z",
     "shell.execute_reply": "2024-11-25T23:47:35.946177Z",
     "shell.execute_reply.started": "2024-11-25T23:47:35.941216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2592000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T23:47:35.948081Z",
     "iopub.status.busy": "2024-11-25T23:47:35.947860Z",
     "iopub.status.idle": "2024-11-25T23:47:35.958627Z",
     "shell.execute_reply": "2024-11-25T23:47:35.957753Z",
     "shell.execute_reply.started": "2024-11-25T23:47:35.948059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Anomalies = 4.896913580246913%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage Anomalies = {(total_anomalies/len(df))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:14:00.265061Z",
     "iopub.status.busy": "2024-11-26T00:14:00.264714Z",
     "iopub.status.idle": "2024-11-26T00:14:00.393864Z",
     "shell.execute_reply": "2024-11-26T00:14:00.392807Z",
     "shell.execute_reply.started": "2024-11-26T00:14:00.265033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/lstm_autoencoder_ha.pth\")\n",
    "np.save(\"/kaggle/working/reconstruction_errors_ha.npy\", reconstruction_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing it on custom Data</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:17:25.535265Z",
     "iopub.status.busy": "2024-11-26T00:17:25.534645Z",
     "iopub.status.idle": "2024-11-26T00:17:25.552184Z",
     "shell.execute_reply": "2024-11-26T00:17:25.551436Z",
     "shell.execute_reply.started": "2024-11-26T00:17:25.535230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Remove \"module.\" prefix if present\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seq_len):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for encoder\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Pass through encoder\n",
    "        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        \n",
    "        # Pass through decoder\n",
    "        decoded, _ = self.decoder(encoded, (hn, cn))\n",
    "        \n",
    "        # Map decoded output back to input size\n",
    "        decoded = self.output_layer(decoded)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "# Initialize the model (ensure the parameters match the saved model)\n",
    "input_size = 20  # Adjust based on your data\n",
    "hidden_size = 256  # Adjust based on your saved model\n",
    "seq_len = 1      # Adjust based on your sequence length\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:17:27.711961Z",
     "iopub.status.busy": "2024-11-26T00:17:27.711601Z",
     "iopub.status.idle": "2024-11-26T00:17:27.725447Z",
     "shell.execute_reply": "2024-11-26T00:17:27.724473Z",
     "shell.execute_reply.started": "2024-11-26T00:17:27.711931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4068981368.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/kaggle/working/lstm_autoencoder_ha.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_Autoencoder(\n",
       "  (encoder): LSTM(20, 256, batch_first=True)\n",
       "  (decoder): LSTM(256, 256, batch_first=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state_dict\n",
    "state_dict = torch.load(\"/kaggle/working/lstm_autoencoder_ha.pth\", map_location=device)\n",
    "\n",
    "# Remove the \"module.\" prefix from the keys\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace(\"module.\", \"\")  # Remove the \"module.\" prefix\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "# Load the updated state_dict into the model\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:17:40.392504Z",
     "iopub.status.busy": "2024-11-26T00:17:40.391844Z",
     "iopub.status.idle": "2024-11-26T00:17:40.462295Z",
     "shell.execute_reply": "2024-11-26T00:17:40.461486Z",
     "shell.execute_reply.started": "2024-11-26T00:17:40.392467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total anomalies: 126928\n"
     ]
    }
   ],
   "source": [
    "reconstruction_errors = np.load(\"/kaggle/working/reconstruction_errors_ha.npy\")\n",
    "\n",
    "threshold = np.percentile(reconstruction_errors, 95)\n",
    "anomalies = reconstruction_errors > threshold\n",
    "\n",
    "total_anomalies = anomalies.sum()\n",
    "print(f\"Total anomalies: {total_anomalies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:17:43.765262Z",
     "iopub.status.busy": "2024-11-26T00:17:43.764541Z",
     "iopub.status.idle": "2024-11-26T00:17:43.770415Z",
     "shell.execute_reply": "2024-11-26T00:17:43.769553Z",
     "shell.execute_reply.started": "2024-11-26T00:17:43.765223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4690455827803817e-05"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:17:47.693393Z",
     "iopub.status.busy": "2024-11-26T00:17:47.692431Z",
     "iopub.status.idle": "2024-11-26T00:17:47.704111Z",
     "shell.execute_reply": "2024-11-26T00:17:47.703239Z",
     "shell.execute_reply.started": "2024-11-26T00:17:47.693350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the scaler from the file\n",
    "sc = load('/kaggle/working/standardscaler_ha.joblib')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T00:18:09.119043Z",
     "iopub.status.busy": "2024-11-26T00:18:09.118340Z",
     "iopub.status.idle": "2024-11-26T00:18:09.127660Z",
     "shell.execute_reply": "2024-11-26T00:18:09.126814Z",
     "shell.execute_reply.started": "2024-11-26T00:18:09.119009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error: 61162.69921875, Anomaly: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Replace with actual new data\n",
    "new_data = np.array([[65,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "# Transform new data using the StandardScaler\n",
    "new_data = sc.transform(new_data)\n",
    "\n",
    "# Convert to tensor and reshape for LSTM\n",
    "new_data_tensor = torch.tensor(new_data, dtype=torch.float32).unsqueeze(0).to(device)  \n",
    "# Shape: (batch_size=1, sequence_length=1, input_size=22)\n",
    "\n",
    "# Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_data_reconstructed = model(new_data_tensor)\n",
    "    reconstruction_error = torch.mean((new_data_tensor - new_data_reconstructed) ** 2).item()\n",
    "\n",
    "# Compare the error with the threshold\n",
    "is_anomaly = reconstruction_error > threshold\n",
    "print(f\"Reconstruction Error: {reconstruction_error}, Anomaly: {is_anomaly}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6063935,
     "sourceId": 9877061,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
