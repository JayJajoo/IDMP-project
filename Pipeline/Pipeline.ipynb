{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seq_len, num_layers=2):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.encoder.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.encoder.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        encoded, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        \n",
    "        decoded, _ = self.decoder(encoded, (hn, cn))\n",
    "        \n",
    "        decoded = self.output_layer(decoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data_rec,comb_dataset, house, House):\n",
    "    #check if missing values \n",
    "    missing_values = data_rec.isnull().sum()\n",
    "    if(len(missing_values[missing_values >0])):\n",
    "        print(missing_values[missing_values >0])\n",
    "        data_rec.fillna(0)\n",
    "    else:\n",
    "        flag=0\n",
    "    if flag==0:\n",
    "        print('No missing values in the record')\n",
    "        \n",
    "    for i in range(1,21):\n",
    "        if data_rec[i] not in [0,1]:\n",
    "            print(\"invalid value assigned to 0\")\n",
    "            data_rec[i]=0\n",
    "    for i in range(21,23):\n",
    "        if (data_rec[i]<1) or (data_rec[i]>27):\n",
    "            print(\"invalid value assigned to 0\")\n",
    "            data_rec[i]=1\n",
    "    #drop columns not needed for lstm\n",
    "    data = data_rec.drop(['Unnamed: 0']) # Remove non-sensor data\n",
    "    # Initialize the model (ensure the parameters match the saved model)\n",
    "    input_size = 25  # Adjust based on your data\n",
    "    hidden_size = 256  # Adjust based on your saved model\n",
    "    seq_len = 1      # Adjust based on your sequence length\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Initialize the model and move it to GPU\n",
    "    model = LSTM_Autoencoder(input_size=input_size, hidden_size=hidden_size, seq_len=seq_len).to(device)\n",
    "    \n",
    "    \n",
    "   # Load the state_dict\n",
    "    state_dict = torch.load(f\"anomaly detection/House_{House}_Model_Files/lstm_autoencoder_{house}.pth\", map_location=device)\n",
    "\n",
    "    # Remove the \"module.\" prefix from the keys\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"module.\", \"\")  # Remove the \"module.\" prefix\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    # Load the updated state_dict into the model\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    reconstruction_errors = np.load(f\"anomaly detection/House_{House}_Model_Files/reconstruction_errors_{house}.npy\")\n",
    "\n",
    "    threshold = np.percentile(reconstruction_errors, 95)\n",
    "    anomalies = reconstruction_errors > threshold\n",
    "\n",
    "\n",
    "    # Load the scaler from the file\n",
    "    sc = load(f'anomaly detection/House_{House}_Model_Files/standardscaler_{house}.joblib')\n",
    "\n",
    "    # Transform new data using the StandardScaler\n",
    "    new_data = sc.transform(np.array([data]))\n",
    "\n",
    "    # Convert to tensor and reshape for LSTM\n",
    "    new_data_tensor = torch.tensor(new_data, dtype=torch.float32).unsqueeze(0).to(device)  \n",
    "    # Shape: (batch_size=1, sequence_length=1, input_size=22)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_data_reconstructed = model(new_data_tensor)\n",
    "        reconstruction_error = torch.mean((new_data_tensor - new_data_reconstructed) ** 2).item()\n",
    "\n",
    "    # Compare the error with the threshold\n",
    "    is_anomaly = reconstruction_error > threshold\n",
    "\n",
    "\n",
    "    if(is_anomaly==True):\n",
    "        print(\"Current Record is an anomaly\")\n",
    "    else:\n",
    "        print(\"Current Record is not an anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the record\n",
      "Current Record is not an anomaly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2y/5kp0bpmn4qnftp03qpmm_5yh0000gn/T/ipykernel_18612/3177742372.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if data_rec[i] not in [0,1]:\n",
      "/var/folders/2y/5kp0bpmn4qnftp03qpmm_5yh0000gn/T/ipykernel_18612/3177742372.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if (data_rec[i]<1) or (data_rec[i]>27):\n",
      "/var/folders/2y/5kp0bpmn4qnftp03qpmm_5yh0000gn/T/ipykernel_18612/3177742372.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"anomaly detection/House_{House}_Model_Files/lstm_autoencoder_{house}.pth\", map_location=device)\n",
      "/Users/jyothssena/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "houseA = pd.read_csv(\"Aras/house_a_combined_dataset.csv\")\n",
    "data_rec= houseA.iloc[0,:]\n",
    "house= 'ha'\n",
    "House= 'A'\n",
    "pipeline(data_rec,houseA, house, House)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
